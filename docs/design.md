# Octo-LLM: 生产级 LLM 接口转换和路由框架

## 设计概述

这是一个生产级的 LLM 接口转换和路由框架，目标是提供一个简单、灵活、可扩展的解决方案，实现不同的 LLM 接口转换为统一的接口，并且能够根据请求路由到不同的 LLM 实例或其余平台的服务。

向外提供的接口支持：
- OpenAI标准的 chat/completions 接口
- OpenAI旧式的 completions 接口
- Anthropic的 messages 接口

其后端对接服务可支持的接口：
- OpenAI标准的 chat/completions 接口，及各种变体
- OpenAI旧式的 completions 接口
- Anthropic的 messages 接口

## 关键概念

1. 模型

指对外提供接口的模型，例如gpt-3.5-turbo、gpt-4、kimi-k2-instruct等。模型名称没有特殊含义，只是一个标识符。

2. 模型服务端点（endpoint）

表明一个具体的 LLM 实例或服务，例如OpenAI的API端点、Anthropic的API端点等。每个模型服务端点都有一个唯一的URL，用于接收请求并返回响应。

3. 路由规则（rule）

指根据请求的模型、接口类型、部分参数信息、请求的AuthN信息等，确定请求应该路由到一个或多个端点。如果是多个端点，则根据负载均衡策略选择一个端点。

4. 中间件（middleware）

通常是对所有请求都生效的组件，目前有下面几种：
- 权限校验：验证用户是否可以访问某个模型。
- 租户/用户对模型的限流：不通用户有不同限流值，包括TPM（Total Tokens Per Minute）、RPM（Requests Per Minute）、RPD（Requests Per Day）。

5. 处理工作流（workflow）

定义对请求和响应内容body的处理。可能的处理节点包括但不限于：
- 输入内容审核：对请求的输入内容进行审核，防止恶意输入或违规内容。
- 输入参数整理和校验：不通接口可能有微小差异，可以自定义参数的处理逻辑。
- 输出内容审核：对返回的输出内容进行审核，防止违规内容。分为对流式、非流式的审核。如果是流式，需要拼小段后再审核。
- 一些特殊情况下处理 usage 和 finish_reason 等字段。
- 拼接流式响应：流式转非流式
- 请求协议格式转换，例如请求是 chat/completions 接口，但是后端服务是 messages。

## 请求处理流程

外界请求到达时，按照下面的流程处理：
1. 按顺序处理中间件。这会完成AuthN、AuthZ，获取限流值，并执行限流等。
2. 根据请求的模型、接口类型、部分参数信息、请求的AuthN信息等，获取一组路由规则。
3. 通过规则引擎逐条处理规则，规则会匹配到一组endpoint和处理工作流。
4. 如果有多个endpoint，根据负载均衡策略选择一个endpoint。
5. 实际转发请求时，使用工作流处理请求和响应内容body，并响应给客户端。

## 一些应用例子

### 接口透传

某个模型直接透传到某个endpoint上，对于请求参数不做任何处理（即使是超出标准的不认识的参数也会透传）。对于响应内容，进行内容审核，记录usage，但不修改响应内容。从而保证从用户视角，模型的行为和直接调用endpoint的行为是一致的。

### 参数归一转发

对于输入参数均进行校验整理，去掉不支持、不认识的参数。对于响应内容，进行内容审核，记录usage，但不修改响应内容。但当响应中出现错误时，隐藏原始接口的错误，并返回一个统一的错误格式和分类。

### 协议转换

后端endpoint是chat/completions接口，但是前端请求是messages接口。

